
## Large Language Model (LLM)

![Pasted image 20251101092704.png](attachments/Pasted%20image%2020251101092704.png)

"large" here refers to the number of parameters

![Pasted image 20251101092815.png](attachments/Pasted%20image%2020251101092815.png)
![Pasted image 20251101093001.png](attachments/Pasted%20image%2020251101093001.png)

## Transformers

![Pasted image 20251101093115.png](attachments/Pasted%20image%2020251101093115.png)


![Pasted image 20251101093323.png](attachments/Pasted%20image%2020251101093323.png)

It goes one step at a time and understands what is happening

solution : 
![Pasted image 20251101094218.png](attachments/Pasted%20image%2020251101094218.png)
![Pasted image 20251101094322.png](attachments/Pasted%20image%2020251101094322.png)


![Pasted image 20251101100231.png](attachments/Pasted%20image%2020251101100231.png)

The above sentence will map to total 15 tokens

![Pasted image 20251101100523.png](attachments/Pasted%20image%2020251101100523.png)

next step embedding

![Pasted image 20251101100553.png](attachments/Pasted%20image%2020251101100553.png)
![Pasted image 20251101101022.png](attachments/Pasted%20image%2020251101101022.png)
![Pasted image 20251101102541.png](attachments/Pasted%20image%2020251101102541.png)

![Pasted image 20251101102903.png](attachments/Pasted%20image%2020251101102903.png)
decoder gives new tokens

![Pasted image 20251101102911.png](attachments/Pasted%20image%2020251101102911.png)

## Prompt Engineering

