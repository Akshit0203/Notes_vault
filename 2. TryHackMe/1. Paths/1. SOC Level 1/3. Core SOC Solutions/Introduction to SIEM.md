# TaskÂ 2 Logs Everywhere, Answers Nowhere

![image-1.png](attachments/image-1.png)

## 1) Host-Centric Log Sources

These log sources capture events that occurred within or related to the host. Devices that generate host-centric logs include Windows,Â Linux, servers, etc. Some examples of host-centric logs are:

- A user accessing a file
- A user attempting to authenticate.
- A process execution activity
- A process adding/editing/deleting a registry key or value.
- PowerShellÂ execution

## 2) Network-Centric Log Sources

Network-related logs are generated when the hosts communicate with each other or access the internet to visit a website. Devices that generate network-centric logs are firewalls,Â IDS/IPS, routers, etc. Some examples of network-centric logs are:

- SSHÂ connection
- A file being accessed viaÂ FTP
- Web traffic
- A user accessing the company's resources throughÂ VPN.
- Network file sharing Activity

Together, these host-centric and network-centric log sources constantly create numerous logs in a network.

## Answers Nowhere

Until now, it seems pretty straightforward that these log sources generate logs, we analyze them, and identify malicious activities. However, it's not that simple. It has some challenges. Some of them are discussed below:

- **Numerous Log Sources**Â A network has many log sources, which generate hundreds of events per second. These logs are scattered across different devices, and examining the logs on each device one by one in case of an incident can be tedious.  
      
    
- **No Centralization**Â As logs reside on the machines on which they are generated,Â  you may need to connect with each log source viaÂ SSH,Â RDP, etc., to analyze logs from multiple log sources. This is very inefficient and can waste a lot of your valuable time during the investigations.  
      
    
- **Limited Context**Â Individual logs cannot tell the whole story of an activity. During any incident, the individual activities on different log sources may seem harmless. But if these logs are correlated, they can indicate a whole different story. For instance, you observed a file access event in a system, which is generally normal activity. However, if you correlate different log sources, you might come to know that this file was accessed by a user who accessed this machine through lateral movement after compromising another machine in the network.  
      
    
- **Limited Analysis**Â The log sources generate numerous logs per second, and analyzing all the logs from all the devices manually to identify any abnormal activity is nearly impossible for humans. Realistically, the analysts will miss a lot of important logs in between the analyses due to their huge number.  
      
    
- **Format Issues**Â Different log sources generate logs in various formats. Analysts need to know all these formats to analyze them, which can be extremely difficult, especially when dealing with numerous log sources in a network.


# TaskÂ 3 Why SIEM?

This is whereÂ SIEMÂ comes into play. Security Information and Event Management (SIEM) is a security solution that collects logs from various types of log sources, standardizes their format into a consistent one, correlates them, and detects malicious activities using detection rules.

## Features ofÂ SIEM

TheÂ SIEMÂ solution not only solves the issues we discussed in the previous task but also provides capabilities to enhance security operations. Let's discuss some of the core features that aÂ SIEMÂ provides.

- **Centralized Log Collection**  
    SIEMÂ collects logs from all sources (endpoints, servers, firewalls, etc.) and centralizes them in one place. These logs are pulled through lightweight agents or APIs and populated into theÂ SIEMÂ solution. This solves the problem of jumping on every machine individually to analyze its logs.Â   
      
    
- **Normalization of Logs**  
    Raw logs are of different formats and sizes. A Windows log does not look the same as aÂ LinuxÂ log. Since aÂ SIEMÂ solution centralizes these logs in one place, it also ensures that all the logs are broken down into different fields and presented in one consistent format. Breaking down a log into several fields for ease of understanding is known as Parsing, and converting all the logs of various log sources into one consistent format is known as Normalization.Â   
      
    
- **Correlation of Logs**  
    Individual logs are not very useful.Â SIEMÂ correlates the logs of different sources and finds any relationship between them. This helps to identify malicious activity by analyzing its pattern. For instance, let's take a look at the following activities happening in a system during a 5-minute timeframe.Â   
    - Haris logs in viaÂ VPNÂ from an IP that he never has previously used
    - Haris accesses some documents on a shared drive
    - Haris executed aÂ PowerShellÂ script
    - The system makes an outbound network connection

Individually assessed, these activities look fine, but theÂ SIEMÂ solution would correlate these activities, which could point to a potential data exfiltration activity resulting from Haris's compromisedÂ VPNÂ credentials.



- **Real-time Alerting  
    **SIEMÂ detects malicious activities based on the rules it contains. Many rules come with aÂ SIEMÂ by default. However, analysts make new detection rules based on their requirements to mature future detections. When the conditions for these detection rules are satisfied, alerts are triggered, and the analysts are notified. Analysts can then investigate these alerts within theÂ SIEMÂ platform.Â Â   
      
    
- **Dashboards and Reporting  
    **Dashboards are the most important components of anyÂ SIEM.Â SIEMÂ presents the data for analysis after being normalized and ingested. The summary of this analysis is presented in the form of actionable insights with the help of multiple dashboards. EachÂ SIEMÂ solution comes with some default dashboards and provides an option for custom Dashboard creation. Below is some of the information that can be found in a dashboard:

- Alert Highlights
- System Notification
- Health Alert
- List of Failed Login Attempts
- Events Ingested Count
- Rules triggered
- Top Domains Visited

An example of a dashboard made inÂ SplunkÂ SIEMÂ is shown below:

![A Splunk dashboard.](https://tryhackme-images.s3.amazonaws.com/user-uploads/6645aa8c024f7893371eb7ac/room-content/6645aa8c024f7893371eb7ac-1753953252149.png)


# TaskÂ 4 Log Sources and Ingestion

## Windows Machine

Windows records every event that can be viewed through the Event Viewer. It assigns a unique ID to each type of log activity, making it easy for the analyst to examine and keep track of. To view events in a Windows environment, typeÂ `Event Viewer`Â in the search bar. This takes you to the tool where different logs are stored and can be viewed, as shown below. These logs from all Windows endpoints are forwarded to theÂ SIEMÂ solution for monitoring and better visibility.

![Shows Events in Event Viewer Interface](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/30beed26fc514cb7f52773b88a4510b9.gif)

## LinuxÂ Machine

LinuxÂ OSÂ stores all the related logs, such as events, errors, warnings, etc. These are then ingested intoÂ SIEMÂ for continuous monitoring. Some of the common locations whereÂ LinuxÂ stores logs are:

- /var/log/httpd: ContainsÂ HTTPÂ RequestÂ  / Response and error logs.
- /var/log/cron: Events related to cron jobs are stored in this location.
- /var/log/auth.log and /var/log/secure: Stores authentication-related logs.
- /var/log/kern: This file stores kernel-related events.

Here is a sample of a cron log:

Cron Logs

```
May 28 13:04:20 ebr crond[2843]: /usr/sbin/crond 4.4 dillon's cron daemon, started with loglevel notice  
May 28 13:04:20 ebr crond[2843]: no timestamp found (user root job sys-hourly)  
May 28 13:04:20 ebr crond[2843]: no timestamp found (user root job sys-daily)  
May 28 13:04:20 ebr crond[2843]: no timestamp found (user root job sys-weekly)  
May 28 13:04:20 ebr crond[2843]: no timestamp found (user root job sys-monthly  
Jun 13 07:46:22 ebr crond[3592]: unable to exec /usr/sbin/sendmail: cron output for user root job sys-daily to /dev/null
```


## Web Server

It is important to monitor all requests/responses coming in and out of the web server for any potential web attack attempt. InÂ Linux, common locations to write allÂ apache-related logs are /var/log/apacheÂ orÂ /var/log/httpd.

Here is an example ofÂ ApacheÂ Logs:

ApacheLogs

```shell-session
192.168.21.200 - - [21/March/2022:10:17:10 -0300] "GET /cgi-bin/try/ HTTP/1.0" 200 3395 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36"  
127.0.0.1 - - [21/March/2022:10:22:04 -0300] "GET / HTTP/1.0" 200 2216 "-" "curl/7.68.0"
```

## Log Ingestion

All these logs provide a wealth of information and can help identify security issues. EachÂ SIEMÂ solution has its own way of ingesting the logs. Some common methods used by theseÂ SIEMÂ solutions are explained below:

1. Agent / Forwarder  
    TheseÂ SIEMÂ solutions provide a lightweight tool called an agent (forwarder byÂ Splunk) that gets installed on the Endpoint. It is configured to capture and send all the important logs to theÂ SIEMÂ server.
<span style="color:rgb(255, 0, 0)">2. <b>Syslog</b>  <br>    Syslog is a widely used protocol to collect data from various systems like web servers, databases, etc., and send real-time data to the centralized destination.</span>
<span style="color:rgb(255, 0, 0)">    ðŸ”Œ How Syslog Sends Logs<br><br>- Uses <b>UDP 514</b> (default, fast but unreliable)<br>    <br>- Can also use <b>TCP 514</b> (reliable, connection-based)</span>
    
- Or **TCP 6514 with TLS** (encrypted)
1. **Manual Upload**  
    SomeÂ SIEMÂ solutions, likeÂ Splunk,Â ELK, etc., allow users to ingest offline data for quick analysis. Once the data is ingested, it is normalized and made available for analysis.
2. **Port-Forwarding**  
    SIEMÂ solutions can also be configured to listen on a certain port, and then the endpoints forward the data to theÂ SIEMÂ instance on the listening port.

An example of howÂ SplunkÂ provides various methods for log Ingestion is shown below:

![Shows data Ingestion Options in Splunk SIEM](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/82d3a3a56537be4635c58cc10caee050.png)


ðŸ”¥ Syslog Severity Levels

|Code|Level|Meaning|
|---|---|---|
|0|Emergency|System unusable|
|1|Alert|Immediate action required|
|2|Critical|Critical error|
|3|Error|Runtime error|
|4|Warning|Warning|
|5|Notice|Normal but significant|
|6|Informational|Info|
|7|Debug|Debug messages|

Example:

`Failed SSH login attempt â†’ severity 4 or 5 Firewall dropped packets â†’ severity 4 Kernel crash â†’ severity 0/1`


|Without Syslog|With Syslog|
|---|---|
|Everyone talks in different languages|Everyone speaks a common language|
|Running to each machine to check logs|One SOC dashboard|
|Hard to detect attacks|Real-time alerts|

ðŸ“Ž Example SOC Use Case

Detect SSH brute force via syslog

Syslog receives:

`Failed password: root from 10.8.12.3 Failed password: root from 10.8.12.3 Failed password: root from 10.8.12.3`

SIEM rule:

`If >10 failures in 5 minutes from same IP â†’ alert HIGH severity`

SOC action:

`Block IP, check success login, investigate`


# TaskÂ 5 Alerting Process and Analysis

- If a user gets five failed Login Attempts in 10 seconds, raise an alert forÂ `Multiple Failed Login Attempts`
- If login is successful after multiple failed login attempts, raise an alert forÂ `Successful Login After multiple Login Attempts`
- A rule is set to alert every time a user plugs in a USB (Useful if USB is restricted as per the company policy)
- If outbound traffic is > 25 MB, raise an alert to potential data exfiltration Attempt (Usually, it depends on the company policy)

## How is a detection rule created?

To explain how the rule works, consider the following Eventlog use cases:

## Use-Case 1:

Adversaries tend to remove the logs during the post-exploitation phase to remove their tracks. <span style="color:rgb(255, 0, 0)">A unique Event IDÂ <b>104</b>Â is logged every time a user tries to remove or clear event logs.</span> To create a rule based on this activity, we can set the condition as follows:

**Rule:**Â If the Log source is WinEventLogÂ **AND**Â EventID isÂ **104**Â - Trigger an alertÂ `Event Log Cleared`

## Use-Case 2:

Adversaries use commands likeÂ `whoami`Â after the exploitation/privilege escalation phase. The following Fields will be helpful to include in the rule.

- Log source: Identify the log source capturing the event logs
- Event ID: Which Event ID is associated with Process Execution activity? In this case, Event ID 4688 will be helpful.
- NewProcessName: Which process name will be helpful to include in the rule?

**Rule:**Â If Log Source is WinEventLogÂ **AND**Â EventCode isÂ **4688,**Â and NewProcessName containsÂ **whoami,**Â then Trigger an ALERTÂ `WHOAMI command Execution DETECTED`


